{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание цен на недвижимость в Калифорнии\n",
    "# Скачан с https://www.kaggle.com/datasets/camnugent/california-housing-prices/data\n",
    "dataset = pd.read_csv(\"housing.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пустые значения есть только в колонке 'total_bedrooms' - заполним их -1\n",
    "dataset['total_bedrooms'] = dataset['total_bedrooms'].fillna(-1)\n",
    "\n",
    "# Выделяем целевую переменную (медианная стоимость дома в ближайшем окружении)\n",
    "feature_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms','total_bedrooms', 'population', 'households', 'median_income','ocean_proximity']\n",
    "target_column = 'median_house_value'\n",
    "data = dataset[feature_columns]\n",
    "target = dataset[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обернем в функции код обучения и валидации качества моделей (линейной регрессии и random forest)\n",
    "# Для оценки качества будем смотреть на MAPE (на сколько процентов в среднем ошиблись) и MAE (на сколько в среднем долларов оишблись)\n",
    "def train_evaluate_lr(train_data, train_target, val_data, val_target):\n",
    "    model = LinearRegression()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Нормализуем данные перед обучением\n",
    "    scaled_train = scaler.fit_transform(train_data)\n",
    "    scaled_val = scaler.transform(val_data)\n",
    "\n",
    "    model = model.fit(scaled_train, train_target)\n",
    "    print(\"Модель обучена\")\n",
    "\n",
    "    val_pred = model.predict(scaled_val)\n",
    "    mape = np.round(mean_absolute_percentage_error(val_target, val_pred) * 100, 2) # Переведем в проценты\n",
    "    mae = int(mean_absolute_error(val_target, val_pred))\n",
    "    print(\"MAPE:\", mape, \"%\")\n",
    "    print(\"MAE:\", mae, \"$\")\n",
    "\n",
    "    # Вычислим средний вклад фичи в предсказание - коэффициент в модели от фичи\n",
    "    coef_dict = {feature: np.round(model.coef_[i], 2) for i, feature in enumerate(train_data.columns)}\n",
    "    sorted_coefs = sorted(coef_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Вклад фичей в предсказание:\")\n",
    "    for feature, coef in sorted_coefs:\n",
    "        print(feature, coef)\n",
    "\n",
    "\n",
    "def train_evaluate_rf(train_data, train_target, val_data, val_target):\n",
    "    model = RandomForestRegressor(max_depth=8)\n",
    "\n",
    "    model = model.fit(train_data, train_target)\n",
    "    print(\"Модель обучена\")\n",
    "\n",
    "    val_pred = model.predict(val_data)\n",
    "    mape = np.round(mean_absolute_percentage_error(val_target, val_pred) * 100, 2) # Переведем в проценты\n",
    "    mae = int(mean_absolute_error(val_target, val_pred))\n",
    "    print(\"MAPE:\", mape, \"%\")\n",
    "    print(\"MAE:\", mae, \"$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бейзлайн без доп фичей (обучаем на чем есть)\n",
    "baseline = data[[feature for feature in feature_columns if feature != \"ocean_proximity\"]]\n",
    "train_data, val_data, train_target, val_target = train_test_split(baseline, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_lr(train_data, train_target, val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим свои фичи на простых функциях и комбинациях\n",
    "basic_feats = data[[feature for feature in feature_columns if feature != \"ocean_proximity\"]]\n",
    "# Вычтем из общего количества комнат количество спален - сколько других комнат\n",
    "basic_feats[\"other_rooms_num\"] = basic_feats['total_rooms'] - basic_feats[\"total_bedrooms\"]\n",
    "\n",
    "# Поделим население на количество домов - сколько в среднем людей живет в одном доме\n",
    "basic_feats[\"avg_people_per_house\"] = basic_feats['population'] / basic_feats[\"households\"]\n",
    "\n",
    "# Умножим население на медианный доход - сколько в среднем зарабатывают всего в районе\n",
    "basic_feats[\"total_income\"] = basic_feats['median_income'] * basic_feats[\"households\"]\n",
    "\n",
    "train_data, val_data, train_target, val_target = train_test_split(basic_feats, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_lr(train_data, train_target, val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возьмем 3 наиболее используемые фичи и разобьем их на бины\n",
    "bin_features = data[[feature for feature in feature_columns if feature not in [\"ocean_proximity\"]]]\n",
    "\n",
    "# Одинаковый размер промежутков - uniform, можно quantile для разделения по квантилям\n",
    "bins = KBinsDiscretizer(n_bins=4, strategy=\"uniform\")\n",
    "bin_features[bins.get_feature_names_out()]= bins.fit_transform(bin_features[[\"latitude\", \"longitude\", \"median_income\"]]).toarray()\n",
    "\n",
    "train_data, val_data, train_target, val_target = train_test_split(bin_features, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_lr(train_data, train_target, val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поработаем с координатами - попробуем добавить расстояние до центров крупных городов Калифорнии\n",
    "# Недвижимость там явно дороже\n",
    "cities = {\n",
    "    \"SC\": (38.57, -121.49),\n",
    "    \"SF\": (37.78, -122.42),\n",
    "    \"SJ\": (37.33, -121.88),\n",
    "    \"LA\": (34.05, -118.24),\n",
    "    \"SD\": (32.71, -117.16),\n",
    "    \"corner\": (41.99, -114.04)\n",
    "}\n",
    "\n",
    "# Вычисляем евклидово расстояние (\"по прямой\"). Корректнее вычислять расстояние по дуге, так как Земля - шар\n",
    "# Haversine - функция, позволяющая географически верно вычислить расстояние\n",
    "def add_dist_to_cities(df, cities):\n",
    "    for city in cities.keys():\n",
    "        df[f\"distance_to_{city}\"] = np.sqrt((df[\"latitude\"] - cities[city][0])**2 + (df[\"longitude\"] - cities[city][1])**2) \n",
    "    \n",
    "    return df\n",
    "\n",
    "dist_features = data[[feature for feature in feature_columns if feature not in [\"ocean_proximity\"]]]\n",
    "dist_features = add_dist_to_cities(dist_features, cities)\n",
    "\n",
    "train_data, val_data, train_target, val_target = train_test_split(dist_features, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_lr(train_data, train_target, val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А теперь посмотрим как добавление фичей влияет на Random Forest\n",
    "print(\"Базовый вариант - без доп фичей\")\n",
    "train_data, val_data, train_target, val_target = train_test_split(baseline, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_rf(train_data, train_target, val_data, val_target)\n",
    "\n",
    "print(\"С фичами из простых функций\")\n",
    "train_data, val_data, train_target, val_target = train_test_split(basic_feats, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_rf(train_data, train_target, val_data, val_target)\n",
    "\n",
    "print(\"С фичами-бинами\")\n",
    "train_data, val_data, train_target, val_target = train_test_split(bin_features, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_rf(train_data, train_target, val_data, val_target)\n",
    "\n",
    "print(\"С фичами расстояний\")\n",
    "train_data, val_data, train_target, val_target = train_test_split(dist_features, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_rf(train_data, train_target, val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценим важности фичей при помощи корреляции\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "corr = r_regression(train_data, train_target, center=True)\n",
    "\n",
    "# Отсортируем и сопоставим с фичами\n",
    "coef_dict = {feature: np.round(abs(corr[i]), 2) for i, feature in enumerate(train_data.columns)}\n",
    "sorted_coefs = sorted(coef_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Вклад фичей в предсказание:\")\n",
    "for feature, coef in sorted_coefs:\n",
    "    print(feature, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_lr(\n",
    "    train_data[[column for column in train_data.columns if column !=\"distance_to_corner\"]],\n",
    "    train_target,\n",
    "    val_data[[column for column in train_data.columns if column !=\"distance_to_corner\"]],\n",
    "    val_target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# У нас есть категориальная фича, которую до этого не использовали\n",
    "dataset[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закодируем двумя методами\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder\n",
    "oh = OneHotEncoder(sparse_output=False)\n",
    "te = TargetEncoder(target_type=\"continuous\")\n",
    "\n",
    "oh_train_data, oh_val_data, train_target, val_target = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "oh_train_data[oh.get_feature_names_out()] = oh.fit_transform(oh_train_data[[\"ocean_proximity\"]])\n",
    "oh_val_data[oh.get_feature_names_out()] = oh.transform(oh_val_data[[\"ocean_proximity\"]])\n",
    "# Убираем оригинальную колонку перед обучением\n",
    "oh_train_data, oh_val_data = oh_train_data.drop(columns=[\"ocean_proximity\"]), oh_val_data.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "te_train_data, te_val_data, train_target, val_target = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "te_train_data[te.get_feature_names_out()] = te.fit_transform(te_train_data[[\"ocean_proximity\"]], train_target)\n",
    "te_val_data[te.get_feature_names_out()] = te.transform(te_val_data[[\"ocean_proximity\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим 2 линейные регрессии:\n",
    "print(\"С One-hot экодером:\")\n",
    "train_evaluate_lr(oh_train_data, train_target, oh_val_data, val_target)\n",
    "\n",
    "print(\"С Target экодером:\")\n",
    "train_evaluate_lr(te_train_data, train_target, te_val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим 2 Random Forest:\n",
    "print(\"С One-hot экодером:\")\n",
    "train_evaluate_rf(oh_train_data, train_target, oh_val_data, val_target)\n",
    "\n",
    "print(\"С Target экодером:\")\n",
    "train_evaluate_rf(te_train_data, train_target, te_val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4c5c14d726543f6ca62e73759dd59cab7817a273324bd9bcaa17f32053a5980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
