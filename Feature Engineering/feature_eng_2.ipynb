{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание цен на недвижимость в Калифорнии\n",
    "# Скачан с https://www.kaggle.com/datasets/camnugent/california-housing-prices/data\n",
    "dataset = pd.read_csv(\"housing.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пустые значения есть только в колонке 'total_bedrooms' - заполним их -1\n",
    "dataset['total_bedrooms'] = dataset['total_bedrooms'].fillna(-1)\n",
    "\n",
    "# Выделяем целевую переменную (медианная стоимость дома в ближайшем окружении)\n",
    "feature_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms','total_bedrooms', 'population', 'households', 'median_income','ocean_proximity']\n",
    "target_column = 'median_house_value'\n",
    "data = dataset[feature_columns]\n",
    "target = dataset[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обернем в функции код обучения и валидации качества моделей (линейной регрессии и random forest)\n",
    "# Для оценки качества будем смотреть на MAPE (на сколько процентов в среднем ошиблись) и MAE (на сколько в среднем долларов оишблись)\n",
    "def train_evaluate_lr(train_data, train_target, val_data, val_target):\n",
    "    model = LinearRegression()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Нормализуем данные перед обучением\n",
    "    scaled_train = scaler.fit_transform(train_data)\n",
    "    scaled_val = scaler.transform(val_data)\n",
    "\n",
    "    model = model.fit(scaled_train, train_target)\n",
    "    print(\"Модель обучена\")\n",
    "\n",
    "    val_pred = model.predict(scaled_val)\n",
    "    mape = np.round(mean_absolute_percentage_error(val_target, val_pred) * 100, 2) # Переведем в проценты\n",
    "    mae = int(mean_absolute_error(val_target, val_pred))\n",
    "    print(\"MAPE:\", mape, \"%\")\n",
    "    print(\"MAE:\", mae, \"$\")\n",
    "\n",
    "    # Вычислим средний вклад фичи в предсказание - коэффициент в модели от фичи\n",
    "    coef_dict = {feature: np.round(abs(model.coef_[i]), 2) for i, feature in enumerate(train_data.columns)}\n",
    "    sorted_coefs = sorted(coef_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Вклад фичей в предсказание:\")\n",
    "    for feature, coef in sorted_coefs:\n",
    "        print(feature, coef)\n",
    "\n",
    "\n",
    "def train_evaluate_rf(train_data, train_target, val_data, val_target):\n",
    "    model = RandomForestRegressor(max_depth=8)\n",
    "\n",
    "    model = model.fit(train_data, train_target)\n",
    "    print(\"Модель обучена\")\n",
    "\n",
    "    val_pred = model.predict(val_data)\n",
    "    mape = np.round(mean_absolute_percentage_error(val_target, val_pred) * 100, 2) # Переведем в проценты\n",
    "    mae = int(mean_absolute_error(val_target, val_pred))\n",
    "    print(\"MAPE:\", mape, \"%\")\n",
    "    print(\"MAE:\", mae, \"$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бейзлайн без доп фичей (обучаем на чем есть)\n",
    "baseline = data[[feature for feature in feature_columns if feature != \"ocean_proximity\"]]\n",
    "train_data, val_data, train_target, val_target = train_test_split(baseline, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_lr(train_data, train_target, val_data, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем кластеризацию и получим номера кластеров для каждого объекта\n",
    "kmeans_feats = data[[feature for feature in feature_columns if feature != \"ocean_proximity\"]]\n",
    "train_data, val_data, train_target, val_target = train_test_split(kmeans_feats, target, test_size=0.2, random_state=42)\n",
    "coord_columns = [\"latitude\", \"longitude\"]\n",
    "\n",
    "# Как и в прошлый раз, расстояния считаем евклидово (можно по haversine)\n",
    "n_clusters = 100\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "train_data[\"cluster\"] = kmeans.fit_predict(train_data[coord_columns])\n",
    "val_data[\"cluster\"] = kmeans.predict(val_data[coord_columns])\n",
    "\n",
    "# Для каждого кластера вычислим среднюю, минимальную и максимальную стоимость домов через groupby\n",
    "# Не забываем, что все это надо вычислять на train чтобы не было лика\n",
    "train_data[\"target\"] = train_target\n",
    "agg_prices = train_data[[\"target\", \"cluster\"]].groupby(\"cluster\").agg([\"mean\", \"min\", \"max\", \"std\"])\n",
    "agg_prices.columns = agg_prices.columns.droplevel(0)\n",
    "agg_prices = agg_prices.add_prefix(\"price_in_cluster_\")\n",
    "\n",
    "# Подсоединяем к нашим данным по кластеру\n",
    "train_data = train_data.merge(agg_prices, on=\"cluster\", how=\"left\")\n",
    "val_data = val_data.merge(agg_prices, on=\"cluster\", how=\"left\")\n",
    "\n",
    "# Из KMeans можно вытащить центройды кластеров\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids_df = pd.DataFrame({\"cluster\": list(range(n_clusters)), \"centroid_lat\": list(centroids[:, 0]), \"centroid_long\": list(centroids[:, 1])})\n",
    "\n",
    "# Посчитаем расстояние от объекта до центра кластера\n",
    "train_data = train_data.merge(centroids_df, on=\"cluster\", how=\"left\")\n",
    "train_data[\"distance_to_center\"] = train_data[[\"latitude\", \"longitude\", \"centroid_lat\", \"centroid_long\"]] \\\n",
    "    .apply(lambda row: np.sqrt((row[\"latitude\"] - row[\"centroid_lat\"])**2 + (row[\"longitude\"] - row[\"centroid_long\"])**2), axis=1)\n",
    "val_data = val_data.merge(centroids_df, on=\"cluster\", how=\"left\")\n",
    "val_data[\"distance_to_center\"] = val_data[[\"latitude\", \"longitude\", \"centroid_lat\", \"centroid_long\"]] \\\n",
    "    .apply(lambda row: np.sqrt((row[\"latitude\"] - row[\"centroid_lat\"])**2 + (row[\"longitude\"] - row[\"centroid_long\"])**2), axis=1)\n",
    "\n",
    "# Убираем лишние колонки\n",
    "train_data = train_data.drop(columns=[\"target\", \"cluster\", \"centroid_lat\", \"centroid_long\"])\n",
    "val_data = val_data.drop(columns=[\"cluster\", \"centroid_lat\", \"centroid_long\"])\n",
    "train_data_kmeans = train_data\n",
    "val_data_kmeans = val_data\n",
    "\n",
    "train_evaluate_lr(train_data_kmeans, train_target, val_data_kmeans, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А теперь посмотрим как добавление фичей влияет на Random Forest\n",
    "print(\"Базовый вариант - без доп фичей\")\n",
    "train_data, val_data, train_target, val_target = train_test_split(baseline, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_rf(train_data, train_target, val_data, val_target)\n",
    "\n",
    "print(\"С фичами c помощью KMeans\")\n",
    "train_evaluate_rf(train_data_kmeans, train_target, val_data_kmeans, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте попробуем добавить признаки от соседей\n",
    "kmeans_feats = data[[feature for feature in feature_columns if feature != \"ocean_proximity\"]]\n",
    "train_data, val_data, train_target, val_target = train_test_split(kmeans_feats, target, test_size=0.2, random_state=42)\n",
    "\n",
    "coord_columns = [\"latitude\", \"longitude\"]\n",
    "\n",
    "# Предполагая что карта разделена на квадраты вида\n",
    "# # #\n",
    "# T #\n",
    "# # #\n",
    "# Попробуем найти 8 и 24 ближайших соседей (1ый и 2ой уровни близости)\n",
    "num_neighbors = [8, 24]\n",
    "for n_neighbors in num_neighbors:\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nn.fit(train_data[coord_columns])\n",
    "    train_data[\"neighbors\"] = list(nn.kneighbors(train_data[coord_columns], return_distance=False))\n",
    "    val_data[\"neighbors\"] = list(nn.kneighbors(val_data[coord_columns], return_distance=False))\n",
    "\n",
    "    # Сделаем функцию для подсчета среднего таргета у соседей\n",
    "    def avg_neigh_price(neigh_indexes):\n",
    "        return train_target[neigh_indexes].mean()\n",
    "\n",
    "    train_target = train_target.reset_index(drop=True)\n",
    "    train_data[f\"{n_neighbors}_neigh_avg_price\"] = train_data[\"neighbors\"].apply(avg_neigh_price)\n",
    "    val_data[f\"{n_neighbors}_neigh_avg_price\"] = val_data[\"neighbors\"].apply(avg_neigh_price)\n",
    "\n",
    "train_data = train_data.drop(columns=[\"neighbors\"])\n",
    "val_data = val_data.drop(columns=[\"neighbors\"])\n",
    "train_data_neigh = train_data\n",
    "val_data_neigh = val_data\n",
    "\n",
    "train_evaluate_lr(train_data_neigh, train_target, val_data_neigh, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А теперь посмотрим как добавление фичей влияет на Random Forest\n",
    "print(\"Базовый вариант - без доп фичей\")\n",
    "train_data, val_data, train_target, val_target = train_test_split(baseline, target, test_size=0.2, random_state=42)\n",
    "train_evaluate_rf(train_data, train_target, val_data, val_target)\n",
    "\n",
    "print(\"С фичами по соседям\")\n",
    "train_evaluate_rf(train_data_neigh, train_target, val_data_neigh, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4c5c14d726543f6ca62e73759dd59cab7817a273324bd9bcaa17f32053a5980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
